---
layout:     post
title:      大语言模型LLMs介绍
subtitle:   由GPT引发的AIGC新趋势
date:       2023-03-14
author:     QITINGSHE
header-img: img/post-bg-unix-linux.jpg
catalog: true
tags:
    - Transformer
    - GPT
---

似乎自从Transformer架构问世之后，NLP领域就进入了一个全新的发展阶段，新的技术突破层出不穷。只是我没有想到像ChatGPT这样的产物能够如此之快的出现在我们的生活中，并在可见的未来对我们的生活产生深远意义的影响。

## 从Transformer到GPT

**涉及到的概念：**

    - MHA（Multi-Head Attention）：多头注意力机制
    - PLM（Pre-trained Language Model）：预训练语言模型
    - LLM（Large Language Model）：大语言模型

本文主要是简略介绍一下现在LLM的一些技术概念和发展，不会过多涉及Transformer的相关知识。

Transformer是由谷歌于2017年提出的处理序列数据的架构，其最重要的概念是注意力机制（Attention Mechanism），通过计算token之间的相关性完成特征的加权融合，同时Transformer支持并行处理序列，大大加快了训练速度。

随后在2018年6月OpenAI就基于Transformer提出了GPT模型，这是一个Decoder-only模型（仅由Transformer的Decoder部分组成），证明了通过**大语料库进行无监督训练**+针对性微调（将监督任务转换成自然语言描述形式）可以得到一个强大的NLP模型，将NLP领域对标注数据的需求极大降低下来。
![](https://github.com/Qitingshe/Qitingshe.github.io/raw/master/_posts/assets/llm1.png)

同年10月，谷歌提出了BERT架构，与GPT不同，BERT是一个Encoder-only架构，也就意味着这是一个双向模型，可以联系上下文语境分析文本，比较擅长完形填空，GPT是一个单向模型，更擅长文本写作。在多个任务上BERT性能都要远好于GPT。即便如此OpenAI依然一股不撞南墙不回头的架势，继续在GPT道路上前进，模型架构没怎么变，但模型规模却做的越来越大，以至于之后由于性能太好，给人一种力大飞砖的感觉。从GPT-2开始，模型出现Zero-Shot特性，可以不经过参数更新在新任务上获得不错的效果。从此OpenAI完成了他的研发逻辑：大语料库+大模型=LLM，这个LLM可以无需参数更新，就可以解决各类实际问题，而且它应该理解人类的自然语言指令，以便人类使用。

## LLMs介绍

近些年的研究表明，在NLP领域，模型扩展（Model Scaling）可以显著提高模型性能，甚至当模型达到一定规模后会出现一些在小模型上不会出现的能力（涌现能力，Emergent ability），如语境学习能力。为了区别于传统的预训练语言模型PLM，学界又提出了新术语——大语言模型LLM。目前LLM的进展已经获得了广泛的关注，而且有理由相信相关技术将会对我们未来的生活产生极大的影响。

<!-- 背景、关键发现、主流技术 -->
<!-- 预训练、适配调整、使用、能力评估 -->

一般LLM是有着上千亿参数量，在海量文本上训练的语言模型，如GPT-3、PaLM这些。LLM可以更好地理解自然语言，并根据给定的上下文（Prompt）生成高质量内容。一般来说，这种能力的提高遵循着[Scaling Laws](https://www.lesswrong.com/posts/midXmMb2Xg37F2Kgn/new-scaling-laws-for-large-language-models)，然后某些能力没法依照Scaling Law进行预测，只有当模型规模超过某个水平才能观察到——涌现能力。

### 涌现能力

涌现能力（Emergent Abilities）是仅在LLM中才会出现的现象，这是LLM区别于PLM的最显著特征：当模型规模达到一定程度，性能获得极大提升，达到类似于人类的开悟状态。涌现能力赋予了LLM以下三种能力：

- 语境学习（In-context learning）：如果模型已经获得了自然语言描述的指令（包括一些任务示例），则模型给予预期输出结果，而无需额外的模型训练（这是一种Zero-shot或Few-shot learning）
- 指令遵循（Instruction following）：通过在格式化的指令数据集上fine-tuning，LLM也表现出用指令形式描述未知任务，并达到不错效果，这大大提高了模型的泛化能力，表现了模型具有理解自然语言指令的能力，寻找合适的Prompt来激发模型能力就变成了一个显性需求
- 逐步推理（Step-by-step reasoning）：在LLM中表现出，可以提供一些解题步骤提示，让模型具有逐步推理的能力，这被称为思维链（CoT，Chain-of-Thought），有人认为这是从代码数据中学习到的

### 扩展定律

从目前的研究总结发现，模型规模的扩展是LLM能力提升的一个关键因素。从GPT-3的175B参数量到PaLM的540B记录，都验证了模型规模的扩展，导致能力的提升。

当然，大的模型尺寸是必不可少的，但是扩展定律并不仅限于此，它一共包括三个方面：
- 模型尺寸（Model size）
- 数据规模（Data size）
- 总计算量（Total compute）

此外，预训练数据的质量在保证模型性能方面有着关键作用，因此在扩展语料库时，要注意数据收集和清理的策略。

## 预训练

预训练决定着LLM的潜力，好的预训练结果才能保证LLM在Fine-tuning阶段爆发强大的能力。而在预训练过程中，预训练语料库的规模和质量又是重中之重。

### 数据收集

LLM对高质量数据有着极强的需求，因为模型的能力很大程度上依赖于预训练语料以及这些数据如何被预处理。预训练语料一般可以被分为两大类：

- 一般数据：如网页，书籍，对话文本；这些数据一般都是数量众多，多样性丰富，而且也比较容易获得
- 专业数据：多语种数据，科研数据，代码；这些数据的引入增强了LLM的泛化能力

一般数据由于数量众多，保证了LLM的建模能力和泛化能力，专业数据虽然数量相对来说要少一点，但是却赋予了LLM在特定任务上的处理问题的能力，进一步扩展了LLM的泛化能力。

对于网页数据来说，数量丰富，容易获得，但是良莠不齐，需要进行精心过滤才能使用。
对话文本可以提升LLM的对话能力，由于对话任务经常涉及到多个参与者，所以会将对话数据转换成树结构，多方会话可以被分解成多个子树。这里还有一个隐患：过度使用对话数据，会影响LLM对自然语言指令的理解能力。
书籍为LLM提供了规范化的长文本数据，增加书籍数据，可以提高LLM从文本中捕获长线依赖的能力，保证了LLM能够生成流畅文本。
专业数据这块重点讲一下代码，代码数据很特殊，它具有很强的逻辑连贯性，一方面引入代码数据，让LLM具有处理代码任务的能力，另一方面，有研究认为思维链（CoT）的推理能力可能就有代码的一定功劳。

**数据过滤**：数据过滤一般有两种方法，一种是基于分类的方法，一种是基于规则的方法；数据过滤的主要目的是为了去除低质量数据。过滤过程中可能会导致有价值的数据丢失，如方言，口语化语言等，导致语料库多样性下降。所以如何更好处理数据是个繁杂的技术活（嗯，这是一个不得不做的dirty work）。
**数据去重**：语料库的重复数据会降低数据集整体的多样性，导致训练过程不稳定，从而影响性能；同时为了避免数据集划分时出现数据重叠问题，去重也是必不可少的工作。
**隐私删除**：还有就是去除敏感信息（个人身份信息等，防止隐私泄露）

当所有数据准备完成后，还需要针对语料库专门设计一个tokenizer，尤其是当这些数据来源不同领域，语种，格式的情况下，这项工作对LLM大有裨益。

### 模型架构

架构这块主要分为三种：
- Encoder-decoder：这个不讲了，华为盘古大模型那种，其实用的人不太多
- Casual Decoder：其实就是Decoder-only，单向生成模型，现在的主流，GPT系列，目前在这个架构上发现了Scaling Law成立，可以通过扩展模型来提升性能
- Prefix Decoder：又称为non-casual decoder，相对于Decoder-only架构而言的，它修改了上面架构中的单向attention mask机制，让prefix token可以进行双向attention，然后生成文本的时候又可以用单向attention

LN在模型中的位置对性能很重要，原始Transformer使用post-LN，但是大部分LLM采用pre-LN，可以保证更稳定的训练，哪怕导致性能下降。后面又出来一些改进版本的Norm方法，比如DeepNorm，据说可以在保证性能的前提下更稳定地保证训练。

激活函数：在LLM中，广泛使用的是GeLU。不过SwiGLU和GeGLU在实践中可以获得更好的表现，但是它们同时也在前馈网络中引入了额外的参数量。

Bias：不加Bias可以提高LLM的训练稳定性

Position Embedding：Transformer中的正弦PE和可学习PE都是绝对PE，但相对PE的embedding是从query和key之间的偏移量获得的，当序列很长的时候（尤其是超过了训练过程中遇到的最大长度），相对PE的性能表现就更好一点

训练这块没啥好讲的，重点是各种并行化（数据并行化，流水线并行化，张量并行化），好像还涉及到混合精度训练，反正也没资源进行训练，不讲了。

## Adaptation Tuning

### Instruction Tuning

### Alignment Tuning

## 怎么用

### In-Context Learning

### Chain-of-Thought Prompting
