---
layout:     post
title:      大语言模型LLMs介绍
subtitle:   由GPT引发的AIGC新趋势
date:       2023-04-19
author:     QITINGSHE
header-img: img/post-bg-unix-linux.jpg
catalog: true
tags:
    - Transformer
    - GPT
---

似乎自从Transformer架构问世之后，NLP领域就进入了一个全新的发展阶段，新的技术突破层出不穷。只是我没有想到像ChatGPT这样的产物能够如此之快的出现在我们的生活中，并在可见的未来对我们的生活产生深远意义的影响。

## 从Transformer到GPT

**涉及到的概念：**

    - MHA（Multi-Head Attention）：多头注意力机制
    - PLM（Pre-trained Language Model）：预训练语言模型
    - LLM（Large Language Model）：大语言模型

本文主要是简略介绍一下现在LLM的一些技术概念和发展，不会过多涉及Transformer的相关知识。

Transformer是由谷歌于2017年提出的处理序列数据的架构，其最重要的概念是注意力机制（Attention Mechanism），通过计算token之间的相关性完成特征的加权融合，同时Transformer支持并行处理序列，大大加快了训练速度。

随后在2018年6月OpenAI就基于Transformer提出了GPT模型，这是一个Decoder-only模型（仅由Transformer的Decoder部分组成），证明了通过**大语料库进行无监督训练**+针对性微调（将监督任务转换成自然语言描述形式）可以得到一个强大的NLP模型，将NLP领域对标注数据的需求极大降低下来。
![](https://github.com/Qitingshe/Qitingshe.github.io/raw/master/_posts/assets/llm1.png)

同年10月，谷歌提出了BERT架构，与GPT不同，BERT是一个Encoder-only架构，也就意味着这是一个双向模型，可以联系上下文语境分析文本，比较擅长完形填空，GPT是一个单向模型，更擅长文本写作。在多个任务上BERT性能都要远好于GPT。即便如此OpenAI依然一股不撞南墙不回头的架势，继续在GPT道路上前进，模型架构没怎么变，但模型规模却做的越来越大，以至于之后由于性能太好，给人一种力大飞砖的感觉。从GPT-2开始，模型出现Zero-Shot特性，可以不经过参数更新在新任务上获得不错的效果。从此OpenAI完成了他的研发逻辑：大语料库+大模型=LLM，这个LLM可以无需参数更新，就可以解决各类实际问题，而且它应该理解人类的自然语言指令，以便人类使用。

## LLMs介绍

近些年的研究表明，在NLP领域，模型扩展（Model Scaling）可以显著提高模型性能，甚至当模型达到一定规模后会出现一些在小模型上不会出现的能力（涌现能力，Emergent ability），如语境学习能力。为了区别于传统的预训练语言模型PLM，学界又提出了新术语——大语言模型LLM。目前LLM的进展已经获得了广泛的关注，而且有理由相信相关技术将会对我们未来的生活产生极大的影响。

<!-- 背景、关键发现、主流技术 -->
<!-- 预训练、适配调整、使用、能力评估 -->

一般LLM是有着上千亿参数量，在海量文本上训练的语言模型，如GPT-3、PaLM这些。LLM可以更好地理解自然语言，并根据给定的上下文（Prompt）生成高质量内容。一般来说，这种能力的提高遵循着[Scaling Laws](https://www.lesswrong.com/posts/midXmMb2Xg37F2Kgn/new-scaling-laws-for-large-language-models)，然后某些能力没法依照Scaling Law进行预测，只有当模型规模超过某个水平才能观察到——涌现能力。

### 涌现能力

涌现能力（Emergent Abilities）是仅在LLM中才会出现的现象，这是LLM区别于PLM的最显著特征：当模型规模达到一定程度，性能获得极大提升，达到类似于人类的开悟状态。涌现能力赋予了LLM以下三种能力：

- 语境学习（In-context learning）：如果模型已经获得了自然语言描述的指令（包括一些任务示例），则模型给予预期输出结果，而无需额外的模型训练（这是一种Zero-shot或Few-shot learning）
- 指令遵循（Instruction following）：通过在格式化的指令数据集上fine-tuning，LLM也表现出用指令形式描述未知任务，并达到不错效果，这大大提高了模型的泛化能力，表现了模型具有理解自然语言指令的能力，寻找合适的Prompt来激发模型能力就变成了一个显性需求
- 逐步推理（Step-by-step reasoning）：在LLM中表现出，可以提供一些解题步骤提示，让模型具有逐步推理的能力，这被称为思维链（CoT，Chain-of-Thought），有人认为这是从代码数据中学习到的

### 扩展定律

从目前的研究总结发现，模型规模的扩展是LLM能力提升的一个关键因素。从GPT-3的175B参数量到PaLM的540B记录，都验证了模型规模的扩展，导致能力的提升。

当然，大的模型尺寸是必不可少的，但是扩展定律并不仅限于此，它一共包括三个方面：
- 模型尺寸（Model size）
- 数据规模（Data size）
- 总计算量（Total compute）

此外，预训练数据的质量在保证模型性能方面有着关键作用，因此在扩展语料库时，要注意数据收集和清理的策略。

## 预训练

预训练决定着LLM的潜力，好的预训练结果才能保证LLM在Fine-tuning阶段爆发强大的能力。而在预训练过程中，预训练语料库的规模和质量又是重中之重。

### 数据收集

LLM对高质量数据有着极强的需求，因为模型的能力很大程度上依赖于预训练语料以及这些数据如何被预处理。预训练语料一般可以被分为两大类：

- 一般数据：如网页，书籍，对话文本；这些数据一般都是数量众多，多样性丰富，而且也比较容易获得
- 专业数据：多语种数据，科研数据，代码；这些数据的引入增强了LLM的泛化能力

一般数据由于数量众多，保证了LLM的建模能力和泛化能力，专业数据虽然数量相对来说要少一点，但是却赋予了LLM在特定任务上的处理问题的能力，进一步扩展了LLM的泛化能力。

对于网页数据来说，数量丰富，容易获得，但是良莠不齐，需要进行精心过滤才能使用。
对话文本可以提升LLM的对话能力，由于对话任务经常涉及到多个参与者，所以会将对话数据转换成树结构，多方会话可以被分解成多个子树。这里还有一个隐患：过度使用对话数据，会影响LLM对自然语言指令的理解能力。
书籍为LLM提供了规范化的长文本数据，增加书籍数据，可以提高LLM从文本中捕获长线依赖的能力，保证了LLM能够生成流畅文本。
专业数据这块重点讲一下代码，代码数据很特殊，它具有很强的逻辑连贯性，一方面引入代码数据，让LLM具有处理代码任务的能力，另一方面，有研究认为思维链（CoT）的推理能力可能就有代码的一定功劳。

**数据过滤**：数据过滤一般有两种方法，一种是基于分类的方法，一种是基于规则的方法；数据过滤的主要目的是为了去除低质量数据。过滤过程中可能会导致有价值的数据丢失，如方言，口语化语言等，导致语料库多样性下降。所以如何更好处理数据是个繁杂的技术活（嗯，这是一个不得不做的dirty work）。
**数据去重**：语料库的重复数据会降低数据集整体的多样性，导致训练过程不稳定，从而影响性能；同时为了避免数据集划分时出现数据重叠问题，去重也是必不可少的工作。
**隐私删除**：还有就是去除敏感信息（个人身份信息等，防止隐私泄露）

当所有数据准备完成后，还需要针对语料库专门设计一个tokenizer，尤其是当这些数据来源不同领域，语种，格式的情况下，这项工作对LLM大有裨益。

### 模型架构

架构这块主要分为三种：
- Encoder-decoder：这个不讲了，华为盘古大模型那种，其实用的人不太多
- Casual Decoder：其实就是Decoder-only，单向生成模型，现在的主流，GPT系列，目前在这个架构上发现了Scaling Law成立，可以通过扩展模型来提升性能
- Prefix Decoder：又称为non-casual decoder，相对于Decoder-only架构而言的，它修改了上面架构中的单向attention mask机制，让prefix token可以进行双向attention，然后生成文本的时候又可以用单向attention

LN在模型中的位置对性能很重要，原始Transformer使用post-LN，但是大部分LLM采用pre-LN，可以保证更稳定的训练，哪怕导致性能下降。后面又出来一些改进版本的Norm方法，比如DeepNorm，据说可以在保证性能的前提下更稳定地保证训练。

激活函数：在LLM中，广泛使用的是GeLU。不过SwiGLU和GeGLU在实践中可以获得更好的表现，但是它们同时也在前馈网络中引入了额外的参数量。

Bias：不加Bias可以提高LLM的训练稳定性

Position Embedding：Transformer中的正弦PE和可学习PE都是绝对PE，但相对PE的embedding是从query和key之间的偏移量获得的，当序列很长的时候（尤其是超过了训练过程中遇到的最大长度），相对PE的性能表现就更好一点。还有一种RoPE的方法，在绝对PE上加上一个旋转矩阵，RoPE可以获得key和query之间的相对位置关系，达到相对PE的作用。LLaMA论文里面也提到了RePE

训练这块没啥好讲的，重点是各种并行化（数据并行化，流水线并行化，张量并行化），好像还涉及到混合精度训练，反正也没资源进行训练，不讲了。

## Adaptation Tuning

经过预训练之后，LLM已经具有一定的解决问题的能力了，但是经过特定任务的fine-tuning之后，LLM会获得更强大的能力。下面主要介绍两种Tuning方法，一种是Insturction Tuning，主要目的是增强或解锁LLM的能力，另一种是Alignment Tuning，是为了让LLM输出的内容符合人类的价值观或偏好。

ChatGPT效果那么好，Adaptation Tuning在其中起着非常重要的作用。

### Instruction Tuning

Instruction Tuning是指在经过预训练的LLM上用格式化的自然语言示例数据集进行fine-tuning，首先当然面对的是收集数据并构建instruction-formatted实例，然后利用这些数据在LLM上训练序列生成任务。经过Instruction tuning之后，LLM在未知任务上的泛化能力将进一步增强，即便在多语言的情况下也是如此。

![](https://github.com/Qitingshe/Qitingshe.github.io/raw/master/_posts/assets/llm2.png)

上图(a)部分向我们展示了instruction-formatted实例的构成，其主体部分是由一个任务描述（Instruction），一个输入-输出对构成，有时候输入文本也会加上少量的演示示例，但这是可选项。

获得instruction-formatted形式的数据集有两种来源，一种是基于已有数据进行转写，如上图(b)，另一种是基于真实人类需求重新撰写，如上图(c)。

上图(b)向我们展示了，根据已有的监督数据集，标注员可以向其中添加自然语言形式的任务描述，并将数据集形式转写成输入-输出对格式。为了增加数据量，还有一些工作尝试将已获取的Instance数据输入-输出逆转，获得新的Instance数据。

虽然转换已有数据集已经可以获得大量的数据了，但是这些数据大部分来自于公开数据集，其多样性可能不够丰富，不一定能够真正符合人类的需求，所以还需要从真实人类的使用需求着手，获得更加可靠的数据集，上图(c)展示了OpenAI如何通过自己的API获取真实用户数据，从现有的服务和标注员手中，得到了真正人类请求后，将这些请求处理成符合要求的任务描述，然后构建输入-输出对。

经过Instruction Tuning后，LLM具有了**理解任务描述的能力**，这是LLM能够在未知任务上有足够泛化能力的原因所在。需要注意的是，在进行Instruction Tuning时，数据量虽然很重要，但是任务的多样性才是LLM能够获得强大泛化能力的关键。甚至当某些方向的任务数据过多还会损害模型表现能力，在构建数据集时要注意平衡不同任务的重要性，而且Fine-tuning的数据量不能过大，否则会影响预训练模型的泛化性能，造成灾难性遗忘（catastrophic forgetting）。当然，在Tuning阶段混合一些预训练阶段的数据可以缓解这个问题。

### Alignment Tuning

如果说Instruction Tuning是激发模型的潜能，让模型具有更好的创造力。那么Alignment Tuning似乎就是在干一件相反的事情，因为LLM具有强大的能力，但是却没有人类的价值观，其输出的内容可能具有谎言、暴力等描述。为了限制模型输出，需要将人类对输出内容的偏好教给模型，让模型有能力识别恶意请求，规范模型输出。

目前比较流行的方案是RLHF，将人类反馈用强化学习的方式接入到模型训练中，这部分内容比较推荐直接看InstructGPT那篇论文。

首先说一下数据，通过对不同的模型输出结果进行人为排序，获取一个输出结果的偏序结果，然后利用Elo评分系统进行打分（防止人为直接打分出现评价标准不一致），在训练时，随机抽出一对结果，让Reward Model学习预测得分。

为了降低人为因素对标注的影响（因为这项标注主观性较大），OpenAI做了很多工作，比如对标注人员的选择，如何保证一致性等。

![](https://github.com/Qitingshe/Qitingshe.github.io/raw/master/_posts/assets/llm3.png)

RLHF的流程分为三个部分有监督fine-tuning、Reward Model训练、PPO训练

**有监督Fine-Tuning**：在一个预训练的GPT-3进行Fine-Tuning，训练数据为\<prompt-response\>对，训练得到SFT模型，用于后续强化学习的训练

**奖励模型训练**：将SFT模型的最后一层unembedding layer去掉，训练一个输入为prompt+response，输出为scalar reward的模型。RM采用一个6B的GPT-3模型，不使用较大的模型一方面是因为大模型会导致训练不稳定，另一方面也会加大计算量

**PPO训练**：利用上面提到的RM对SFT模型的输出进行打分，作为reward，完成PPO训练。PPO算法是利用reward对模型的选择行为直接进行增强或减弱，好的行为会增加下一次被选中的概率，坏的行为会被削弱下一次被选中的概率。PPO阶段不需要人工参与

经过RLHF训练后的模型，其输出内容获得非常高的接受度，从下图可以发现，1.3B的模型效果要比175B的好
![](https://github.com/Qitingshe/Qitingshe.github.io/raw/master/_posts/assets/llm4.png)

另外不得不提一嘴，进行Alignment Tuning可能会对模型的泛化性能造成一定影响，作者把这个现象称为alignment tax，所以说如何用更灵活的方式扩展现有的训练体系，以有效地支持数据更新是未来需要考虑的一个研究方向

## 怎么用

从数据收集到预训练，再到后面的Fine-Tuning，终于完成了一个合格的LLM的训练。前面我们提到了，LLM不同于PLM的一个最大区别是LLM具有涌现能力。
![](https://github.com/Qitingshe/Qitingshe.github.io/raw/master/_posts/assets/llm5.png)

### In-Context Learning

ICL首次在GPT-3中出现，如上图左边所示，ICL使用格式化的自然语言prompt，一般包括任务描述和一些示例，形成具有特定格式的LLM输入，基于这样的输入，LLM可以理解并完成新任务，而**无需梯度更新**。

其实到这里可以发现ICL和Instruction Tuning有着非常紧密的关系，Instruction Tuning需要进行fine-tune LLM，但是ICL仅需要使用prompt去激发LLM的能力即可，无需更新模型参数。目前很多研究发现，ICL的性能受演示示例的影响很大。ICL经常因不同的示例而有很大的差异。所以现在出现一些prompt工程师的岗位，专门有人去研究这个问题去了。

[A Survey on In-context Learning](https://arxiv.org/abs/2301.00234)这篇文章对ICL有着更加详细的介绍，有需求的可以进一步去查阅一下。

### Chain-of-Thought Prompting

CoT其实是ICL的一个特例，相当于在ICL的基础上增加了推理步骤。目前发现对同一个问题，给予不同的推理路径，可以有效提升模型性能，我猜可能一题多解的描述方式可以让模型更好的理解任务需求。还有一个发现说，越复杂的推理路径越可能引发出LLM的推理能力，可能是更详细的描述增加了信息量。不管怎么说，LLM的推理能力是让人非常惊艳的能力，给人一种它具有思考力的感觉。正如前面数据整理阶段所述，CoT的能力很有可能是跟训练数据中的代码数据有关，因为代码数据的逻辑性很强，能够帮助LLM获得推理能力（虽然目前还没有足够的实验证据来佐证这一观点）。但是已经有一些实验证据证明，Instruction Tuning不是导致CoT的关键因素

## 收尾

本文主要介绍了LLM的一些特性，比如Scaling Law，涌现能力这些。很明显，AI领域发展到当下这个阶段，模型架构的作用似乎并不如数据本身的作用更大，那么拥有更多数据，更多算力的机构在这波浪潮中的优势是很明显的。目前也有很多工作在尝试降低LLM的训练门槛，对我个人来说，可能更期待LLM在专业领域的应用，可能不需要庞大的模型，也不需要强大的多领域泛化能力。那么在现在，设计合理高效的方式去收集有价值的数据可能是更加重要的事情。

另外，微软在[Sparks of Artificial General Intelligence: Early experiments with GPT-4](https://arxiv.org/abs/2303.12712)中说GPT-4:

> we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system

这句话给人一种奇点临近的感觉，倘若真的如此，那么现有的Alignment Tuning这种方式，真的能向LLM赋予人类的价值观吗？如果有一天AGI真的出现，我们是无法得知它是真的严格遵循人类价值观运行，还是仅仅伪装成一个听话的AI来哄骗人类。阿西莫夫的机器人三定律大家都知道，但是影视作品中，“不得伤害人类”这些价值观是硬编码在机器人的底层核心中的。但是经过Alignment Tuning所获的的价值观还具有这么强的约束力吗？也难怪前段时间大佬们集体联名要求暂停AI实验了。[X-Risk Analysis for AI Research](https://arxiv.org/abs/2206.05862)这篇文章给了更多有关AI风险的探讨。

怎么更好的约束LLM的输出在未来一段时间应该也会成为重点关注的方向，如果想要商业化应用，如何管住AI的嘴，这是避无可避的一个问题。

## 其它

- 有关LLM中Prompt的介绍：[Prompt Engineering Guide](https://github.com/dair-ai/Prompt-Engineering-Guide)
- wolfram关于ChatGPT的介绍：[What Is ChatGPT Doing … and Why Does It Work?](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/)
- 伯克利的Koala：[Koala: A Dialogue Model for Academic Research](https://bair.berkeley.edu/blog/2023/04/03/koala/)
- LLM医疗领域的一些尝试：[ChatDoctor](https://github.com/Kent0n-Li/ChatDoctor)
