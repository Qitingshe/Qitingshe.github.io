---
layout:     post
title:      ResNet介绍
subtitle:   残差网络，模型退化
date:       2018-05-21
author:     QITINGSHE
header-img: img/post-bg-debug.png
catalog: true
tags:
    - CNN
    - DeepLearning
---

# 什么是残差网络

ResNet：Residual Networks是MSRA何凯明团队在2015年提出的，在多项比赛中获得第一名的成绩，其相关论文 `Deep Residual Learning for Image Recognition` 也获得了CVPR2016的最佳论文。论文中作者提出了Residual的结构

![](https://github.com/Qitingshe/Qitingshe.github.io/raw/master/_posts/assets/Residualblock.png)

即增加了一个identity mapping（恒等映射），将原来需要学习的函数 $H(x)$ 转换成$F(x)+x$ 文章认为，相比较于$H(x)$ ，$F(x)$ 的优化要简单，二者的优化难度并不相同，这一想法源自图像处理中的**残差向量编码**，通过一个reformulation，将一个问题分解成多个尺度之间的残差问题，可以更好的获得优化效果。

# 网络退化

在深度学习中，随着网络的加深，网络的优化问题变得越来越困难，研究表明网络的深度是实现更好效果的重要因素，然而梯度弥散/梯度爆炸成为训练深层网络的障碍，导致无法收敛。虽然可以通过归一化解决问题，但网络却开始退化了，即增加网络层数却导致更大的误差，这种deep plainnet 的收敛率十分低下。![degradation](https://github.com/Qitingshe/Qitingshe.github.io/raw/master/_posts/assets/resnet-degradation.png)

假设我们在一个浅层的网络基础上添加一个identity mapping恒等映射，可以保证网络的深度在加深，但因为没有学习新的参数，网络不会发生退化的现象。

实际结果表明，多层非线性网络无法逼近恒等映射网络。

但是不退化不是我们的最终目的，我们希望有更好的网络性能，resnet学习的残差函数$F(x)=H(x)-x$ ，这里倘若$ F(x)=0$ ，这就是之前的恒等映射，而实际情况是，resnet的短路连接是恒等映射的特素情况，它没有引入额外的参数和计算复杂度。假如优化目标函数$H(x)$ 是逼近一个恒等映射，而不是0映射，那么学习找到恒等映射的扰动项$F(x)$ 会比重新学习一个映射函数要容易。如下图，残差函数一般会有较小的响应波动，表明恒等映射是一个合理的预处理

![](https://github.com/Qitingshe/Qitingshe.github.io/raw/master/_posts/assets/resnet-shortcut.png)

# Residual block

残差块 Residual block通过shortcut connection实现，通过shortcut将block的输入输出进行一个简单的叠加，这个操作不会给网络增加额外的参数和计算量，却可以大大增加模型的训练速度，提高训练效果，当模型的深度增加时，该结构能够很好的解决退化问题。

残差块的结构：

![](https://github.com/Qitingshe/Qitingshe.github.io/raw/master/_posts/assets/Residualblock.png)

一共有两层，表达式如下，其中$\sigma$ 代表非线性函数ReLU
$$
\mathcal{F}(x)=W_2\sigma(W1*x)
$$
然后通过一个shortcut，和第二个ReLU连接，得到输出y：
$$
y=\mathcal{F}(x,{W_i})+x
$$
当需要对输入和输出维度进行变化时（如改变通道数），可以在shortcut时对x做一个线性变换$W_s$
$$
y=\mathcal{F}(x,{W_i})+W_s*x
$$
然而实验证明x已经足够，不需要添加一个维度变换，除非在某些特定维度的输出，需要进行通道翻倍。

实验表明，残差块往往需要两层以上，一层的残差块 $y=W_1*x+x$ 并不能起到提升的作用

**总结:** 相比较于学习原始特征$\mathcal{H}(x)=\mathcal{F}(x)+x$ 我们学习其残差$\mathcal{F}(x)=\mathcal{H}(x)-x$ 要容易。如果我们学习的残差$\mathcal{F}(x)=0$ ，此时堆积层仅仅作了恒等映射，至少保证网络性能不会下降，实际上残差不会为0, 这会使得堆积层在输入特征的基础上学习到新的特征，从而获得更好的性能 



## 实验部分

![resnet](https://github.com/Qitingshe/Qitingshe.github.io/raw/master/_posts/assets/resnet.jpeg)

构建了一个18层和34层的plain网络作为对比，所有层都只作简单的叠加，之后又构建了一个18层和34层的residual网络，在plain网络上加入shortcut，两个网络的参数量和计算量相同，与VGG19相比，计算量小很多。这是该模型最大的优势所在。

实验发现，在plain上观测到明显的退化现象，而在ResNet上没有出现退化现象，34层的网络反而比18层的更好，同时ResNet的收敛速度比plain要快。

![resnet](https://github.com/Qitingshe/Qitingshe.github.io/raw/master/_posts/assets/resnetvsplain.png)

## shortcut

对于短路连接shortcut的方式，作者提出三个选项：

- 使用恒等映射，如果residual block的输入输出不一致，对增加的维度用0来填充
- 在block输入输出的维度一致时使用恒等映射，不一致时使用线性投影以保证维度一致
- 对所有的block均使用线性投影

对三个方法的实验表明，第三个好于第二个好于第一个，但是差距不明显，因此认为线性投影不是必须的，而使用0填充可以降低模型的复杂度

## deeper residual block

对于更深的网络，文章提出了新的residual block，考虑到计算成本，对深层的网络，将两个3×3的卷积层替换为1×1+3×3+1×1的卷积层，新结构中的3×3卷积首先在一个降维1×1卷积层下减少了计算量，然后在另一个1×1的卷积层下做了还原，既保持了精度又减少了计算量（降维程度为输入输出的1/4）。

![block](https://github.com/Qitingshe/Qitingshe.github.io/raw/master/_posts/assets/resnetblock.png)

作者提出了更深的网络，模型不仅没有出现退化的现象，错误率也大大降低，同时计算复杂度也保持在很低的程度。































