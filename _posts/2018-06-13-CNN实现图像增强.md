---
layout:     post
title:      Deep Bilateral Learning for Real-Time Image Enhancement 
subtitle:   从 bilateral filter 到 HDRnet
date:       2018-06-08
author:     QITINGSHE
header-img: img/post-bg-coffee.jpeg
catalog: true
tags:
    - 深度学习
    - 数字图像处理
    - 神经网络
---

# HDRNet

本文提出的网络架构可以应用在丰富多彩的摄影图片增强工作中，而且可以在高分辨率输入图片上快速计算，这得益于我们采用的三个策略：

- 切片：slicing
- 仿射颜色变换：affine color transform
- 全分辨率损失函数：full-resolution loss

## slicing

我们将大部分的预测工作放在低分辨率的双边网格中进行，其中，对每个像素的x，y坐标添加第三个维度，这是像素颜色函数，为此在深度学习中引入新的节点，perform a data-dependent lookup。这就是所谓的切片操作。通过考虑除了x,y坐标之外的每个像素的输入颜色来从3维双边网格重建全分辨率的输出图像。

## affine color transform

通过以前的研究工作，发现相比较与直接预测输出图像，预测从输入到输出的转换函数通常要更简单一点。所以本文的结构设计为学习作为中间表示的局部仿射颜色变换，通过引入新的乘法节点来将其应用到输入图像

## full-resolution loss

尽管大多数学习都是在低分辨率下完成，但是训练使用的损失函数是在全分辨率下进行评估的，这样在低分辨率下学习到的转换可直接通过对高分辨率图像的影响来进行优化

# ARCHITECTURE

本文提出一个新的卷积网络结构，经过训练可以完成快速图像增强，该模型具有保边效果。

![hdrnet](https://github.com/Qitingshe/Qitingshe.github.io/raw/master/_posts/assets/hdrnet.png)

## 低分辨率

先将图片分辨率降低，通过多个卷积层对其进行处理，然后执行低分辨率参数预测，得到一个双边网格参数$A$，用来表示局部仿射变换。按经验而言，图像增强通常不仅取决于局部图像特征，还取决于全局图像特征，如直方图、平均强度、甚至场景类别。因此我们将低分辨率图像，进一步分解为本地路径$L^i$和全局路径$G^i$，然后将两条路径合并得到$F$，以产生最终的表示仿射变换的系数。全局和本地共享一组共同的低级特征$S^i$

具体来说，low-res input $\bar I$ 具有固定维度256×256，第一步操作就是将其通过一系列步长为2的卷积层$(S^i)_{i=1,2....n_s}$ 以抽取低级特征，同时降低空间分辨率，这里的激活函数为ReLU。之后最后一个低级特征将被分解为两条不对称路径分别处理，第一条路径$L^i$ 是一个全卷积网络，专门用来学习在保留空间信息的情况下传播图像的局部特征；第二条路径$G^i$ 使用卷积和全连接层，通过覆盖整张低分辨率图像$\bar I$ 的感受野来学习全局特征(如高级场景类别，室内/室外等)的一个固定尺寸向量。然后将两个路径的输出$G^{n_G}$ 和$L^{n_L}$ 合并成一组共同的特征$F$ ，一个Pointwise linear layer将混合流输出为最终的矩阵$A$ 。我们将这个矩阵视为仿射系数的双边网格。

### low-lever features

这部分的层级有两个作用：

- 驱动低分辨率输入$\bar I$ 与最终的仿射系数网格之间的空间下采样
- 控制着预测的复杂度，越深的层有着指数级更大的空间支持，有着更加复杂的非线性能力，这样可以抽取更加复杂的特征

### Local features path

一共两层卷积，步长为1，在局部路径保持空间分辨率和特征数不变，一个足够深的卷积网络对捕获有用的语义特征至关重要，如果最终的系数网格需要更高的空间分辨率，则可以减小$n_s$ 并增加$n_l$ 以相应地进行补偿，以便不降低网络的表现能力，没有局部路径，预测系数就会失去任何空间位置的概念。

### Global features path

一共5层，其中前两层为步长为2的卷积层，后面接3个全连接层。全局路径产生一个64维的矢量来总结全局信息，并作为一个事先信息来规范由局部路径作出的局部决策。如果没有全局特征来对输入的高级描述进行编码，那么网络将会作出导致伪像的错误局部决策

### Fusion and liner prediction

这部分采用逐点仿射混合将局部和全局路径融合，同时采用了一个ReLU激活函数


$$
F_c[x,y]=\sigma(b_c+\sum\limits_{c'}w'_{cc'}G^{n_G}_{c'}+\sum\limits_{c'}w_{cc'}L^{n_L}_{c'}[x,y])
$$


这样就产生了一个16×16×64的特征矩阵，由此再经过一个1×1的线性预测，生成一个16×16×96的映射：


$$
A_c[x,y]=b_c+\sum\limits_{c'}F_{c'}[x,y]w_{cc'}
$$


详细的网络架构如下：

![hdrnet](https://github.com/Qitingshe/Qitingshe.github.io/raw/master/_posts/assets/hdrnetarchi.png)

$F$ 处维持通道为64, $A$ 处就变换到96了。



### 作为双边网格的图像特征

为了方便解释，我们将最后的feature map $A$ 看作一个多通道双边网格，该网格的第三维将会被展开：
$$
A_{dc+z}[x,y]\leftrightarrow A_c[x,y,z]
$$
其中$d=8$ 表示网格的深度，在这种解释下，$A$ 可以视为一个16×16×8的双边网格，每个网格有12个数，代表一个3×4仿射色彩变换矩阵的系数。这样的reshape操作，比在网格上简单使用3D卷积更有表现力，因为3D卷积仅仅完成$z$ 方向的局部连接。它比标准的双边网格划分更具表现力，标准双边网格将$I$ 离散化为几个强度单元，然后使用box filters对结果进行处理。

该操作易于使用2层网络表达，从某种意义上说，通过在整个过程中保持二维卷积公式，并且只将最后一层解释为双边网格，我们让网络决定什么时候2D到3D  是最佳的。

## 高分辨率

在高分辨率数据流下，以全分辨率工作，这部执行很小的计算，主要用于获取高频信息，在保留边缘方面发挥重要作用。它学习一个灰度引导图$g$，然后网络中引入一个切片节点，使用该引导图将仿射系数网格$A$上采样回一个全分辨率$\bar A$ ，这些pre-pixel局部仿射变换之后将会应用于全分辨率输入，从而产生最终的输出$O$。

在训练过程中，在全分辨率下最小化损失函数，这意味着只处理严重下采样的数据仍然可以学习到可重现高频效果的中间特征和仿射系数。

### Guidance map auxiliary network

我们将 $g$ 定义为全分辨率特征的逐点非线性变换：


$$
g[x,y]=b+\sum\limits_{c=0}^2\rho_c(M_c^\top\centerdot\phi_c[x,y]+b_c')\\
\rho_c(x)=\sum\limits_{i=0}^{15}a_{c,i}max(x-t_{c,i},0)
$$
$M_c^\top$ 是3×3的颜色转换矩阵，下图给出颜色转换矩阵和每个通道的颜色曲线

![ctm](https://github.com/Qitingshe/Qitingshe.github.io/raw/master/_posts/assets/hdrctm.png)

### Upsampling with a trainable slicing layer

这里介绍下如何恢复高解析度图像，我们先引入一个slicing layer，它基于双边网格完成切片操作，该层将引导图$g$ 和双边网格$A$ 作为输入，它在最终的feature map $A$ 执行一个data-dependent lookup，之后得到一个与$g$ 空间分辨率一样的feature map $\bar A$ ，这是由下式对由$g$ 定义的位置对 $A$ 的系数做三次线性插值获得的：


$$
\bar A_c[x,y]=\sum\limits_{i,j,k}\tau(s_xx-i)\tau(s_yy-j)\tau(d\centerdot g[x,y]-k)A_c[i,j,k]
$$

线性插值核函数是$\tau(\centerdot)=max(1-\|\centerdot\|,0)$ ，$s_x,s_y$ 是网格与全分辨率图像的宽高比，实际上每个像素都被赋予一个系数向量，其在网格中的深度取决于灰度引导图$g[x,y]$ （公式中：$d\centerdot g[x,y]$）

### Assembling the final output

从全尺度图像来看，图像操作可能很复杂，但近期研究发现，尽管复杂的图像处理pipelines也能精确地建模为一系列简单局部转换的集合，因此我们将最终输出$O_c$ 的每个通道建模为全分辨率特征的仿射组合，其系数由sliced feature map $\bar A$ 来定义。


$$
O_c[x,y]=\bar A_{n_\phi+(n_\phi+1)c}+\sum\limits_{c'=0}^{n_\phi-1}\bar A_{c'+(n_\phi+1)c}[x,y]\phi_{c'}[x,y]
$$


# 损失函数
数据集为$D=\{I_i,O_i\}$
损失函数为L2损失:


$$
L=\frac{1}{|D|}\sum|I_i-O_i|^2
$$

- weight decay:$10^{-8}$
- 优化器：Adam solver
- batch size:受分辨率影响，4～16不等、
- learning rate:$10^{-4}$
- epoch:30

同时在每个feature map之间增加BN，整个训练大概花了2～3天