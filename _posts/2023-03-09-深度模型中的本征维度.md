---
layout:     post
title:      深度模型中的本征维度
subtitle:   本征维度，模型微调
date:       2023-03-09
author:     QITINGSHE
header-img: img/post-bg-debug.png
catalog: true
tags:
    - CNN
    - DeepLearning
---

时隔五年，又重新捡起了博客记录，本次介绍一下[[2012.13255] Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning (arxiv.org)](https://arxiv.org/abs/2012.13255)。

最近一段时间各种大模型层出不穷，不乏有着像Stable Diffusion和ChatGPT这样火出圈外的。大模型的流行，也给下游任务的fine-tune带来了麻烦，如何更经济、更高效地训练大模型，成为急需解决的问题，为此出现一些新的工作，如Adapter-based方法、Prefix-based方法以及Lora等。那么何以这些方法可以解决大模型fine-tune问题，以及模型在下游任务中得以有效fine-tune的内在原理是什么呢？为什么只用及少量的数据就可以利用预训练模型完成某个下游分支任务的fine-tune？

本文将介绍一下Intrinsic Dimension(本征维度)，以及从Intrinsic Dimension的角度来思考以上问题。

## 什么是Intrinsic Dimension

首先我们丢出几个概念，Intrinsic Dimension是什么？

**Intrinsic Dimension**：An objective function’s intrinsic dimensionality describes the minimum dimension needed to solve the optimization problem it defines to some precision level.

如果一个大模型是将数据映射到高维空间进行处理，这里假定在处理一个细分的小任务时，是不需要那么复杂的大模型的，可能只需要在某个子空间范围内就可以解决，那么也就不需要对全量参数进行优化了，现实中我们难以精确找到某个问题所对应的子空间，但是我们可以定义当对某个子空间参数进行优化时，能够达到全量参数优化的性能的一定水平（如90%精度）时，那么这个子空间所对应的维度就可以称为对应当前待解决问题的Intrinsic Dimension。

**d90**：已知某个目标任务，并且知道对应该任务的全量参数fine-tune所能达到的精度。如果只更新某个子空间参数，使其能达到全量参数精度的90%，则称能达到该要求的所有子空间中，维度最小的那个子空间所对应的维度为$d_{90}$

有了这些基础概念，下面再丢几个本文介绍的几个结论：
 1. 预训练隐式地降低了Intrinsic Dimension
 2. 大模型在经过一定次数训练后，倾向于有着更低的Intrinsic Dimension
 3. 越简单的下游任务，有着越低的Intrinsic Dimension
 4. 越低的Intrinsic Dimension，有着越好的泛化性能

上面结论4其实是可以和结论1进行互相印证的，其中一个成立，另一个也就不言自明了。

假定预训练模型的参数为$\theta_0^D$，本征维度（全量参数的某个子空间）上的参数为$\theta^d$

通过矩阵$M$将本征维度映射到预训练模型的全尺度维度上

$$\theta^D=\theta_0^D+\theta^d M$$

然后固定预训练模型参数，只针对本征维度上参数进行特定任务的finetune训练，如果能达到$d_{90}$，即相比较全量参数参与训练，如果本征维度的参数训练能达到其性能的90%即可满足要求。（LoRA就是采用了类似的思想，它用了矩阵$A$和$B$代替的上面提到的$M$，而且LoRA因为是线性设计，所以可以将参数融合到原始模型中，达到可以不增加计算量的条件下完成不同任务的学习。）

## 实验部分

### 实验1
![id1](https://github.com/Qitingshe/Qitingshe.github.io/raw/master/_posts/assets/id1.png)

从上图中可以看到，在两个不同数据集上，分别对四个模型进行训练，其中水平虚线表示各个模型的$d_{90}$精度。
可以发现，随着参与训练的子空间维度变大（参数量变多），模型的整体性能是在不断提升的；同时观察各个模型的$d_{90}$可以发现，同类模型中，更大的模型有着更低的$d_{90}$，这就验证了结论2。

### 实验2

![block1](https://github.com/Qitingshe/Qitingshe.github.io/raw/master/_posts/assets/id2.png)

从上图中可以看到，在同一个模型上针对6个不同数据集进行测试，我们发现随着训练次数的增加，$d_{90}$在不断下降，这就验证了结论1

### 实验3

![block2](https://github.com/Qitingshe/Qitingshe.github.io/raw/master/_posts/assets/id3.png)

文章作者还在某个数据集上测试了不同模型的$d_{90}$，然后发现随着模型参数量的增加，模型的$d_{90}$整体是呈一个下降趋势的。这也验证了结论2，说明模型参数量和$d_{90}$存在负相关性。

### 实验4

![block2](https://github.com/Qitingshe/Qitingshe.github.io/raw/master/_posts/assets/id4.png)
![block2](https://github.com/Qitingshe/Qitingshe.github.io/raw/master/_posts/assets/id5.png)
上面第一张图说明了$d_{90}$与精度的关系，第二张图说明$d_{90}$与泛化Gap的关系，从图中可以看出，越低的$d_{90}$有着越高的精度，换句话说，越低的$d_{90}$有着越好的泛化性，其泛化Gap越小，验证了结论4。

## 总结

本文介绍了Intrinsic Dimension，并通过实验说明了几个问题，作者认为从Intrinsic Dimension的角度来说，大模型具有更强的信息压缩能力，也就是说经过一段时间训练，可以得到更低的Intrinsic Dimension——当模型参数越多的时候，我们需要用来表示一个任务的信息量少（因为可以在一个更低维的子空间中进行任务的学习）。

这片文章其实蛮有趣的，从一个比较新的角度来理解模型优化问题，而且为大模型的优化提供了一些理论上的参考。
