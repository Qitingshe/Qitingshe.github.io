---
layout:     post
title:      神经网络模型压缩和加速
subtitle:   模型分析
date:       2018-04-25
author:     QITINGSHE
header-img: img/post-bg-debug.png
catalog: true
tags:
    - 模型压缩和加速
    - DeepLearning
---

- 为了解决全连接层的参数规模问题，人们转而考虑增加卷积层，使全连接层参数降低，随之而来的负面影响是大大增长了计算时间与能耗

Krizhevsky 在2014年提出：卷积层占据了大约90-95%的计算时间，且参数规模有较大的值，全连接层占据了大约5-10%的计算时间，95%的参数规模，并且值较小

具有50个卷积层的ResNet-50 需要超过95MB的存储器以及38亿次浮点计算。当丢弃一些冗余权重后，网络仍然正常工作，但节省了超过75%的参数和50%的计算时间。

# 现有的深度模型压缩方法

- 参数修剪和共享：parameter pruning and sharing --> 针对模型参数的冗余性,试图去除冗余和不重要的项。
- 低秩因子分解：low-rank factorization --> 使用矩阵/张量分解估计深度学习模型的信息参数。
- 转移/紧凑卷积滤波器：transferred/compact convolutional filters --> 设计了特殊的结构卷积滤波器来降低存储和计算复杂度。
- 知识蒸馏：knowledge distillation --> 学习一个蒸馏模型，训练一个更紧凑的神经网络来重现一个更大的网络的输出。












[深度神经网络模型压缩和加速](https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247488630&idx=1&sn=894b06c31b37ccdad3e9bfdd7323a33f&chksm=96e9cbf6a19e42e0c666d6727430a39fe4e09db047c3cfc0465a34923b87a36dfbe7585fe339&mpshare=1&scene=1&srcid=0424MGFyXMFRJJhmO0qtb6mP#rd)